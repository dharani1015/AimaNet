{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttentionMask(nn.Module):\n",
        "    \"\"\"\n",
        "    Calculate attention over the provided input.\n",
        "    Check the dim over which sum needs to be taken.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(AttentionMask, self).__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # check: the dimension over which summation needs to be taken. If input tensor is [N, H, W, C] (tensorflow). Then its over spatial dimensions. Change correspondngly.\n",
        "        xsum = torch.sum(x, dim=2, keepdim=True)\n",
        "        xsum = torch.sum(xsum, dim=3, keepdim=True)\n",
        "        xshape = x.size()\n",
        "        return (x/xsum) * xshape[1] * xshape[2] * 0.5\n",
        "\n",
        "\n",
        "class TSM(nn.Module):\n",
        "    \"\"\"\n",
        "    Temporal shift module\n",
        "    Tensor order (tensorflow)- shape=(N, H, W, C)\n",
        "    Tensor order (pytorch)- torch.Size([N, C, H, W])\n",
        "    \"\"\"\n",
        "    def __init__(self, n_frame, fold_div=3):\n",
        "        super(TSM, self).__init__()\n",
        "        self.n_frame = n_frame\n",
        "        self.fold_div = fold_div\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #  shape\n",
        "        b, c, h, w = x.shape\n",
        "        # reshape\n",
        "        x = torch.reshape(x, (-1, self.n_frame, c, h, w))\n",
        "        fold = c // self.fold_div\n",
        "        last_fold = c - (self.fold_div - 1) * fold\n",
        "        # split based on channel dimension. SC has axis=-1\n",
        "        out1, out2, out3 = torch.split(x, [fold, fold, last_fold], dim=2)\n",
        "\n",
        "        # shift left\n",
        "        padding_1 = torch.zeros_like(out1)\n",
        "        # last frame\n",
        "        padding_1 = padding_1[:, -1, :, :, :]\n",
        "        # introduce dimension for frames again\n",
        "        padding_1 = padding_1.unsqueeze(dim=1)\n",
        "        _, out1 = torch.split(out1, [1, self.n_frame-1], dim=1)\n",
        "        out1 = torch.cat((out1, padding_1), dim=1)\n",
        "\n",
        "        # shift right\n",
        "        padding_2 = torch.zeros_like(out2)\n",
        "        # first frame\n",
        "        padding_2 = padding_2[:, 0, :, :, :]\n",
        "        # introduce dimension for frames again\n",
        "        padding_2 = padding_2.unsqueeze(dim=1)\n",
        "        out2, _ = torch.split(out2, [self.n_frame-1, 1], dim=1)\n",
        "        out2 = torch.cat((padding_2, out2), dim=1)\n",
        "\n",
        "        # concatenate outs over channel axis\n",
        "        out = torch.cat((out1, out2, out3), dim=2)\n",
        "        out = torch.reshape(out, (-1, c, h, w))\n",
        "        return out\n",
        "\n",
        "\n",
        "class TemporalShiftModuleConvolution(nn.Module):\n",
        "    def __init__(self, n_frame, in_channels, out_channels, kernel_size=(3,3), padding=\"same\", activation=\"tanh\"):\n",
        "        super(TemporalShiftModuleConvolution, self).__init__()\n",
        "        self.tsm = TSM(n_frame=n_frame)\n",
        "        # padding = \"same\", check if this is going to give problems if trying on onxx\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding)\n",
        "        if activation == \"tanh\":\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == \"relu\":\n",
        "            self.activation = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.tsm(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Conv2DWithActivation(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=128, kernel_size=(3,3,), padding=\"valid\", activation=\"tanh\"):\n",
        "        super(Conv2DWithActivation, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding)\n",
        "        if activation == \"tanh\":\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == \"sigmoid\":\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif activation == \"relu\":\n",
        "            self.activation = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TemporalShiftCAN(nn.Module):\n",
        "    def __init__(self, n_frame, in_channels, out_channels_1, out_channels_2, kernel_size=(3,3), hidden_size=128):\n",
        "        super(TemporalShiftCAN, self).__init__()\n",
        "\n",
        "        # TSM convolution for motion data\n",
        "        self.tsm_conv1 = TemporalShiftModuleConvolution(n_frame=n_frame, in_channels=in_channels, out_channels=out_channels_1, \n",
        "                                                        kernel_size=kernel_size, padding=\"same\", activation=\"tanh\")\n",
        "        self.tsm_conv2 = TemporalShiftModuleConvolution(n_frame=n_frame, in_channels=out_channels_1, out_channels=out_channels_1, \n",
        "                                                        kernel_size=kernel_size, padding=\"valid\", activation=\"tanh\")\n",
        "\n",
        "        # regular convolution on appearance data\n",
        "        self.reg_conv1 = Conv2DWithActivation(in_channels=in_channels, out_channels=out_channels_1, kernel_size=kernel_size, \n",
        "                                              padding=\"same\", activation=\"tanh\")\n",
        "        self.reg_conv2 = Conv2DWithActivation(in_channels=out_channels_1, out_channels=out_channels_1, kernel_size=kernel_size, \n",
        "                                              padding=\"valid\", activation=\"tanh\")\n",
        "\n",
        "        # gated convolution 1\n",
        "        self.g1_conv = Conv2DWithActivation(in_channels=out_channels_1, out_channels=1, kernel_size=(1,1), padding=\"same\", activation=\"sigmoid\")\n",
        "        # Attention mask\n",
        "        self.attention_mask = AttentionMask()\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=(2,2))\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "        # TSM Covolution\n",
        "        self.tsm_conv3 = TemporalShiftModuleConvolution(n_frame=n_frame, in_channels=out_channels_1, out_channels=out_channels_2, \n",
        "                                                        kernel_size=kernel_size, padding=\"same\", activation=\"tanh\")\n",
        "        self.tsm_conv4 = TemporalShiftModuleConvolution(n_frame=n_frame, in_channels=out_channels_2, out_channels=out_channels_2, \n",
        "                                                        kernel_size=kernel_size, padding=\"valid\", activation=\"tanh\")\n",
        "\n",
        "        # regular convolution\n",
        "        self.reg_conv3 = Conv2DWithActivation(in_channels=out_channels_1, out_channels=out_channels_2, kernel_size=kernel_size, padding=\"same\", \n",
        "                                              activation=\"tanh\")\n",
        "        self.reg_conv4 = Conv2DWithActivation(in_channels=out_channels_2, out_channels=out_channels_2, kernel_size=kernel_size, padding=\"valid\", \n",
        "                                              activation=\"tanh\")\n",
        "\n",
        "        # gated convolution 2\n",
        "        self.g2_conv = Conv2DWithActivation(in_channels=out_channels_2, out_channels=1, kernel_size=(1,1), padding=\"same\", activation=\"sigmoid\")\n",
        "\n",
        "        # FC layers\n",
        "        # check this in feature size\n",
        "        self.fc1 = nn.Linear(in_features=out_channels_2*7*7, out_features=hidden_size)\n",
        "        self.fc2 = nn.Linear(in_features=hidden_size, out_features=1)\n",
        "        self.final_activation = nn.Tanh()\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x_motion, x_appearance):\n",
        "        # x_motion: [10, 3, 36, 36], x_appearance: [10,3,36,36]\n",
        "        d1 = self.tsm_conv1(x_motion)\n",
        "        d2 = self.tsm_conv2(d1)\n",
        "\n",
        "        r1 = self.reg_conv1(x_appearance)\n",
        "        r2 = self.reg_conv2(r1)\n",
        "\n",
        "        g1 = self.g1_conv(r2)\n",
        "        g1 = self.attention_mask(g1)\n",
        "        gated1 = torch.mul(d2, g1)\n",
        "\n",
        "        d3 = self.avgpool(gated1)\n",
        "        d4 = self.dropout(d3)\n",
        "\n",
        "        r3 = self.avgpool(r2)\n",
        "        r4 = self.dropout(r3)\n",
        "\n",
        "        d5 = self.tsm_conv3(d4)\n",
        "        d6 = self.tsm_conv4(d5)\n",
        "\n",
        "        r5 = self.reg_conv3(r4)\n",
        "        r6 = self.reg_conv4(r5)\n",
        "\n",
        "        g2 = self.g2_conv(r6)\n",
        "        g2 = self.attention_mask(g2)\n",
        "        gated2 = torch.mul(d6, g2)\n",
        "\n",
        "        d7 = self.avgpool(gated2)\n",
        "        d8 = self.dropout(d7)\n",
        "\n",
        "        # d9 = torch.flatten(d8)\n",
        "        d9 = d8.view(d8.shape[0], -1)\n",
        "        d10 = self.fc1(d9)\n",
        "        d10 = self.final_activation(d10)\n",
        "        d11 = self.dropout2(d10)\n",
        "        out = self.fc2(d11)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "zwROGYv_76oV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ts_can_model = TemporalShiftCAN(n_frame=10, in_channels=3, out_channels_1=32, out_channels_2=64, kernel_size=(3,3), hidden_size=128)\n",
        "ts_can_model = ts_can_model.to(device)"
      ],
      "metadata": {
        "id": "PsZK_iGW73ie"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wp = \"/content/drive/MyDrive/ProjectT/AIMA/Copy of best_model_checkpoint_2022-12-14.pt\"\n",
        "ts_can_model.load_state_dict(torch.load(wp)[\"model_state_dict\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZvcMWDY8B50",
        "outputId": "446b16d6-e10f-4fb3-efde-89b9e6cb2c28"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(ts_can_model,\"/content/drive/MyDrive/ProjectT/AIMA/ts_can_model.pth\")"
      ],
      "metadata": {
        "id": "er3WCkoQ-qNU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp = \"/content/drive/MyDrive/ProjectT/AIMA/ts_can_model.pth\"\n",
        "# # Load the model\n",
        "model = torch.load(mp)"
      ],
      "metadata": {
        "id": "EcPoZnf3q7fR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = (torch.randn(10, 3, 36, 36).cuda(), torch.randn(10, 3, 36, 36).cuda())"
      ],
      "metadata": {
        "id": "TDblOtrGGA5t"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model(*inp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxqbfzTKF_7e",
        "outputId": "c5ebfb7b-52e6-4fa3-e554-c1ed6a0a9368"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9.32 ms, sys: 0 ns, total: 9.32 ms\n",
            "Wall time: 15.8 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0903],\n",
              "        [-0.1455],\n",
              "        [ 0.0922],\n",
              "        [ 0.0976],\n",
              "        [-0.1110],\n",
              "        [-0.0203],\n",
              "        [-0.1504],\n",
              "        [-0.1649],\n",
              "        [-0.1716],\n",
              "        [ 0.0096]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.quantization\n",
        "\n",
        "# Quantize the model\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    # MyModel(), \n",
        "    model, \n",
        "    dtype=torch.quint8,\n",
        ")\n",
        "\n",
        "torch.save(quantized_model, \"/content/drive/MyDrive/ProjectT/AIMA/quantized_model.pth\")"
      ],
      "metadata": {
        "id": "pNhIAtU0prTq"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "quantized_model(*inp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygXYqE_8GSgr",
        "outputId": "33d07e88-1015-49cb-c920-c89ccf5e214b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.77 ms, sys: 0 ns, total: 7.77 ms\n",
            "Wall time: 9.6 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0397],\n",
              "        [-0.0394],\n",
              "        [-0.0397],\n",
              "        [-0.0399],\n",
              "        [-0.0394],\n",
              "        [-0.0394],\n",
              "        [-0.0392],\n",
              "        [-0.0399],\n",
              "        [-0.0397],\n",
              "        [-0.0397]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Optional Pruning.\n",
        "\n",
        "# pruning_params = {\n",
        "#     'pruning_method': 'l1_unstructured',\n",
        "#     'sparsity': 90,\n",
        "#     'pruning_freq': 5,\n",
        "# }\n",
        "\n",
        "# # Prune the model\n",
        "# pruned_model = torch.optim.prune(quantized_model, pruning_params)"
      ],
      "metadata": {
        "id": "ZTTJxC8PxjZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use torch.jit.trace to convert the model to a torch.jit.ScriptModule object\n",
        "inp = (torch.randn(10, 3, 36, 36).cuda(), torch.randn(10, 3, 36, 36).cuda())\n",
        "model = torch.jit.trace(quantized_model, inp)\n",
        "model.save(\"/content/drive/MyDrive/ProjectT/AIMA/mobile_model.pth\")"
      ],
      "metadata": {
        "id": "D55xTtd4lBaE"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Load the model\n",
        "model = torch.jit.load(\"/content/drive/MyDrive/ProjectT/AIMA/mobile_model.pth\")"
      ],
      "metadata": {
        "id": "UTZJNML8lDMA"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "output = model(*inp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4y4xj4GFyHf",
        "outputId": "f35d2364-5c7a-4c88-db96-a73ed9bc8b78"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 32 ms, sys: 0 ns, total: 32 ms\n",
            "Wall time: 32.1 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model on the mobile device's CPU or GPU\n",
        "# mobile_output = mobile_model(torch.randn(input_shape)).eval()"
      ],
      "metadata": {
        "id": "RWbXhoWtqiFZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
